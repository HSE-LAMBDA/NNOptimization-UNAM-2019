{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1-scikit-search.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "nbconvert_exporter": "python",
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "version": "3.5.2",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      },
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "outputHidden": false,
        "inputHidden": false,
        "id": "_j55y1q8aA44",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Scikit-learn model optimisation\n",
        "\n",
        "## Start from good old GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "5vyzOQaIaA46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# by Andrey Ustyuzhanin with heavy scikit-learn documentation re-use\n",
        "# Kudos to Raghav RV <rvraghav93@gmail.com>\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.datasets import make_hastie_10_2\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "pXVoFICYaA48",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = make_hastie_10_2(n_samples=8000, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EfOQ3vorNN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's check X shape\n",
        "print (<YOUR CODE>) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "biHLC7-taA4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# let's start with simple 1D grid of numbers from 2 to 402 with step 10\n",
        "param_grid ={'min_samples_split': <YOUR CODE>} "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "KdtK2vYqaA5C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The scorers can be either be one of the predefined metric strings or a scorer\n",
        "# callable, like the one returned by make_scorer\n",
        "scoring = {'AUC': 'roc_auc', \n",
        "          }\n",
        "\n",
        "# Setting refit='AUC', refits an estimator on the whole dataset with the\n",
        "# parameter setting that has the best cross-validated AUC score.\n",
        "# That estimator is made available at ``gs.best_estimator_`` along with\n",
        "# parameters like ``gs.best_score_``, ``gs.best_parameters_`` and\n",
        "# ``gs.best_index_``\n",
        "gs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                  param_grid=param_grid,\n",
        "                  scoring=scoring, cv=5, refit='AUC',\n",
        "                  return_train_score=True)\n",
        "gs.fit(X, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEG73r_HehST",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's examine keys available inside 'cv_results_'\n",
        "print (\"\\n\".join(gs.cv_results_.keys()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "NzCd0GQ8aA5E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_optimisation(results, scoring, param_name):\n",
        "  fig = plt.figure(figsize=(8, 8))\n",
        "\n",
        "  plt.title(\"GridSearchCV evaluating\", fontsize=14)\n",
        "\n",
        "  plt.xlabel(param_name)\n",
        "  plt.ylabel(\"Score\")\n",
        "\n",
        "  ax = plt.axes()\n",
        "  y_min, y_max = 1e10, -1e10\n",
        "\n",
        "  # Get the regular numpy array from the MaskedArray\n",
        "  X_axis = np.array(results['param_' + param_name].data, dtype=float)\n",
        "\n",
        "  for scorer, color in zip(sorted(scoring), ['g', 'k']):\n",
        "      for sample, style in (('train', '--'), ('test', '-')):\n",
        "          sample_score_mean = results['mean_%s_%s' % (sample, scorer)]\n",
        "          sample_score_std = results['std_%s_%s' % (sample, scorer)]\n",
        "          ax.fill_between(X_axis, sample_score_mean - sample_score_std,\n",
        "                          sample_score_mean + sample_score_std,\n",
        "                          alpha=0.1 if sample == 'test' else 0, color=color)\n",
        "          ax.plot(X_axis, sample_score_mean, style, color=color,\n",
        "                  alpha=1 if sample == 'test' else 0.7,\n",
        "                  label=\"%s (%s)\" % (scorer, sample))\n",
        "          y_max = max(np.max(sample_score_mean + 1.5 * sample_score_std), y_max)\n",
        "          y_min = min(np.min(sample_score_mean - 1.5 * sample_score_std), y_min)\n",
        "          \n",
        "\n",
        "      best_index = np.nonzero(results['rank_test_%s' % scorer] == 1)[0][0]\n",
        "      best_score = results['mean_test_%s' % scorer][best_index]\n",
        "\n",
        "      # Plot a dotted vertical line at the best score for that scorer marked by x\n",
        "      ax.plot([X_axis[best_index], ] * 2, [0, best_score],\n",
        "              linestyle='-.', color=color, marker='x', markeredgewidth=3, ms=8)\n",
        "\n",
        "      # Annotate the best score for that scorer\n",
        "      ax.annotate(\"%0.2f\" % best_score,\n",
        "                  (X_axis[best_index], best_score + 0.005))\n",
        "\n",
        "  ax.set_ylim(y_min, y_max)\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgzaPyoAbiAi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_optimisation(gs.cv_results_, scoring, list(param_grid.keys())[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHpGgNr3kucX",
        "colab_type": "text"
      },
      "source": [
        "So far, so good. Let's add `accuracy_score` as extra  metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ges4ZI_zk85d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "scoring = {'AUC': 'roc_auc', \n",
        "           'accuracy': make_scorer(<ACCURACY FUNCTION>)\n",
        "          }\n",
        "\n",
        "gs = GridSearchCV(DecisionTreeClassifier(random_state=42),\n",
        "                  param_grid=param_grid,\n",
        "                  scoring=scoring, cv=5, refit='AUC',\n",
        "                  return_train_score=True)\n",
        "gs.fit(X, y);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ln_Z9OplWeb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_optimisation(gs.cv_results_, scoring, list(param_grid.keys())[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJmZF2Q0aA5G",
        "colab_type": "text"
      },
      "source": [
        "## RandomizedSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "ggXzc0fNaA5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import randint as sp_randint\n",
        "from time import time\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# get some data\n",
        "digits = load_digits()\n",
        "X, y = digits.data, digits.target\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cxI9m6Lrsls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's check X and X[0] shapes\n",
        "print(<YOUR CODE>) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "gKzdAGrKaA5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(X[0].reshape(8,8)); "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "j4DqWZLpaA5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Utility function to report best scores\n",
        "def report(results, n_top=3):\n",
        "    for i in range(1, n_top + 1):\n",
        "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
        "        for candidate in candidates:\n",
        "            print(\"Model with rank: {0}\".format(i))\n",
        "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
        "                  results['mean_test_score'][candidate],\n",
        "                  results['std_test_score'][candidate]))\n",
        "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
        "            print(\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "vc4HCA6AaA5Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a classifier\n",
        "clf = RandomForestClassifier(n_estimators=20)\n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "param_dist = {\"max_depth\": [3, None],\n",
        "              \"max_features\": sp_randint(1, 11),\n",
        "              \"min_samples_split\": sp_randint(2, 11),\n",
        "              \"min_samples_leaf\": sp_randint(1, 11),\n",
        "              \"bootstrap\": [True, False],\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "uZvCs7EmaA5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run randomized search\n",
        "n_iter_search = 20\n",
        "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
        "                                   n_iter=n_iter_search)\n",
        "\n",
        "start = time()\n",
        "random_search.fit(X, y)\n",
        "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
        "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
        "report(random_search.cv_results_)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLxt9b-8aA5h",
        "colab_type": "text"
      },
      "source": [
        "### Compare with GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "7V3Ew8oYaA5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(clf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "outputHidden": false,
        "inputHidden": false,
        "id": "67xmkKaqaA5l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# use a full grid over all parameters\n",
        "param_grid = {\"max_depth\": [3, None],\n",
        "              \"max_features\": <YOUR CODE>, # logarithmic from 1 to 10\n",
        "              \"min_samples_split\": [2, 3, 10],\n",
        "              \"min_samples_leaf\": [1, 3, 10],\n",
        "              \"bootstrap\": <YOUR CODE>, # either True or False\n",
        "              \"criterion\": [\"gini\", \"entropy\"]}\n",
        "\n",
        "# run grid search\n",
        "grid_search = GridSearchCV(clf, param_grid=param_grid)\n",
        "start = time()\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "print(\"GridSearchCV took %.2f seconds for %d candidate parameter settings.\"\n",
        "      % (time() - start, len(grid_search.cv_results_['params'])))\n",
        "report(grid_search.cv_results_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VKJUPEKpz3r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}